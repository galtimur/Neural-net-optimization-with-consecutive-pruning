{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"NJLxv3yZbryy","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676660216696,"user_tz":-60,"elapsed":1757,"user":{"displayName":"Timur","userId":"09007836499695604677"}},"outputId":"beda563c-6f2f-43c1-8a03-35a3f2d47891"},"outputs":[{"output_type":"stream","name":"stdout","text":["rm: cannot remove '/content/Neural-net-optimization-with-consecutive-pruning': No such file or directory\n","Cloning into 'Neural-net-optimization-with-consecutive-pruning'...\n","remote: Enumerating objects: 50, done.\u001b[K\n","remote: Counting objects: 100% (50/50), done.\u001b[K\n","remote: Compressing objects: 100% (49/49), done.\u001b[K\n","remote: Total 50 (delta 26), reused 0 (delta 0), pack-reused 0\u001b[K\n","Unpacking objects: 100% (50/50), 17.09 KiB | 1.42 MiB/s, done.\n"]}],"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","@author: Timur Galimzyanov\n","\"\"\"\n","!rm -r /content/Neural-net-optimization-with-consecutive-pruning\n","!git clone https://github.com/galtimur/Neural-net-optimization-with-consecutive-pruning"]},{"cell_type":"markdown","source":["Importing libraries and defining seed everything function"],"metadata":{"id":"WK_DdLz0cFPq"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"10C1c1efbry5"},"outputs":[],"source":["import numpy as np\n","import torch\n","import torch.nn as nn\n","import matplotlib.pyplot as plt\n","\n","import random\n","from torch.utils.data import DataLoader\n","from sklearn.model_selection import train_test_split\n","from torch.utils.data import TensorDataset\n","from tqdm import tqdm\n","import torch.optim.lr_scheduler as sched\n","from torch.utils.data.dataloader import default_collate\n","\n","import gc\n","\n","import sys\n","sys.path.insert(1, '/content/Neural-net-optimization-with-consecutive-pruning')\n","\n","# Loading pruning functions\n","from prun_functions import prune_model, reprune_model_local, unprune_model, count_non_zero\n","# Loading MLP models and train data generation functions\n","from mlp_setup import Teacher, Student, gen_train\n","\n","# Importing functions for plotting results and e.t.c.\n","from plotting_results import plot_train, loss_landscape1D, loss_landscape2D, plot_landscape1D, plot_landscape2D\n","\n","device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","\n","def seed_everything(seed):\n","\n","    '''\n","    Seed\n","    '''\n","\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    torch.cuda.manual_seed(seed)\n","    torch.cuda.manual_seed_all(seed)\n","    torch.backends.cudnn.benchmark = False\n","    torch.backends.cudnn.deterministic = True"]},{"cell_type":"markdown","source":["Testing pruning functions"],"metadata":{"id":"7zEgzQQ9eQFZ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Iy2nHEfHbry7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1676660226840,"user_tz":-60,"elapsed":4332,"user":{"displayName":"Timur","userId":"09007836499695604677"}},"outputId":"67b6e194-cc8f-44ce-c4c1-926b8a20cf48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Amount of nonzero params = 0.900\n","Amount of nonzero params = 0.800\n","Amount of nonzero params = 0.700\n","Amount of nonzero params = 0.600\n","Amount of nonzero params = 0.500\n","Amount of nonzero params = 0.400\n","Amount of nonzero params = 0.300\n","Amount of nonzero params = 0.200\n"]}],"source":["model = Student(n_in = 30, n_out = 5, hidden = 20)\n","model.to(device)\n","pruns = np.arange(0.9, -0.01, -0.1)\n","pruns = np.arange(0.1, 0.9, 0.1)\n","\n","for i, alpha in enumerate(pruns):\n","\n","    if i == 0:    \n","        prune_model(model, alpha=alpha)\n","    else:\n","        reprune_model_local(model, alpha=alpha)\n","\n","    print(f'Amount of nonzero params = {count_non_zero(model):.3f}')"]},{"cell_type":"markdown","source":["Traning function. Trains model n_epochs.\n","\n","Returns timeseries of losses - train, validation and OOD"],"metadata":{"id":"lyNNBAz1cgxC"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"7gS4dFNhbry8"},"outputs":[],"source":["def train(model, train_loader, optimizer, device, n_epochs = 5, loss_fun = nn.L1Loss(), scheduler = None):\n","\n","    '''\n","    Traning function\n","    Returns timeseries of losses - train, validation and OOD\n","    '''\n","\n","    losses = []\n","    losses_val = []\n","    losses_ODD = []\n","    \n","    #loss_fun = nn.CosineEmbeddingLoss()\n","    model.train()\n","\n","    for epoch in range(n_epochs):#tqdm(range(n_epochs)):\n","    \n","        for features, targets in train_loader:\n","        \n","            res = model(features.to(device))\n","            loss = loss_fun(res, targets.to(device)) #, torch.ones(len(targets)).to(device)\n","            \n","            with torch.no_grad():   \n","                res_val = model(X_test.to(device))\n","                loss_val = loss_fun(res_val, y_test.to(device)).item()\n","                losses_val.append(loss_val/val_norm)\n","                \n","                res_ODD = model(train_ODD.to(device))\n","                loss_ODD = loss_fun(res_ODD, target_ODD.to(device)).item()\n","                losses_ODD.append(loss_ODD/ODD_norm)\n","                \n","            model.zero_grad()\n","            loss.backward()\n","            losses.append(loss.item()/train_norm)\n","            optimizer.step()\n","        \n","        if scheduler != None:\n","            scheduler.step()\n","    \n","    return losses, losses_val, losses_ODD"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AeZLbhskbry-","executionInfo":{"status":"ok","timestamp":1676660226841,"user_tz":-60,"elapsed":37,"user":{"displayName":"Timur","userId":"09007836499695604677"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"51f8fdd8-845d-402c-f1ab-4454a486e66e"},"outputs":[{"output_type":"stream","name":"stdout","text":["base loss =  0.09941038295408124\n"]}],"source":["train_samples, target_orig, train_ODD, target_ODD = gen_train(hidden_size = 200, new = True)\n","n_in = train_samples.size()[1]\n","n_out = target_orig.size()[1]\n","\n","## Addition of the random noise to the target\n","alpha = 0.1\n","std = torch.std(target_orig)\n","target = target_orig + alpha*std*torch.randn_like(target_orig)\n","\n","var = torch.var(target_ODD)\n","target_ODD = target_ODD + alpha*var*torch.randn_like(target_ODD)\n","\n","## L1 norm of the original data to normalize L1 loss.\n","train_orig_norm = (abs(target_orig)).mean().item()\n","\n","loss_fun = nn.L1Loss()\n","print('base loss = ', loss_fun(target, target_orig).item()/train_orig_norm)\n","\n","seed_everything(42)\n","\n","X_train, X_test, y_train, y_test = train_test_split(train_samples, target, test_size = 0.1, train_size = 0.9)\n","train_set = TensorDataset(X_train, y_train)\n","train_loader = DataLoader(train_set, batch_size=256, shuffle=True)\n","\n","#train_norm = (abs(y_train)).mean().item()\n","#val_norm = (abs(y_test)).mean().item()\n","val_norm = train_orig_norm\n","train_norm = train_orig_norm\n","\n","ODD_norm = (abs(target_ODD)).mean().item()"]},{"cell_type":"markdown","source":["Functions for model training"],"metadata":{"id":"xHpkwTjSdQyi"}},{"cell_type":"code","source":["def train_model(train_loader, device, seed=42, hidden_size = 100, plot_text=''):\n","\n","    seed_everything(seed)\n","    \n","    model = Student(n_in = n_in, n_out = n_out, hidden = hidden_size)\n","    #model = torch.load('teacher.pt')\n","    model.to(device)\n","    \n","    optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-2, weight_decay=0.01)\n","    # optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n","    \n","    scheduler = sched.MultiStepLR(optimizer, milestones=[2,3,4,5,6,7,8,9,10], gamma=0.7)\n","    #scheduler = None\n","    \n","    res = train(model, train_loader, optimizer, device, n_epochs = 2, scheduler = scheduler)\n","    print(f'Amount of nonzero params = {count_non_zero(model):.3f}')\n","    plot_train(res, 2, plot_text)\n","    \n","    # model_copy = copy.deepcopy(model)\n","    return model"],"metadata":{"id":"J2yagUIzdJhj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Functions for model training at consequative prunning levels "],"metadata":{"id":"RFNb1D4pdWBz"}},{"cell_type":"code","source":["def train_pruning(pruns, train_loader, device, seed=42, hidden_size = 100, n_epochs=5,\n","                  plot_text='', to_plot = True):\n","    \n","    global result\n","    \n","    seed_everything(seed)\n","    \n","    model = Student(n_in = 30, n_out = 5, hidden = hidden_size)\n","    #model = torch.load('teacher.pt')\n","    model.to(device)\n","\n","    # optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n","       \n","    optimizer = torch.optim.AdamW(model.parameters(), lr = 5e-3, weight_decay=0.00) \n","    \n","    # unprune_model(model)\n","    res_train = []\n","    res_val = []\n","    res_OOD = []\n","    gamma = 1\n","\n","    for i, alpha in enumerate(pruns):\n","        #seed_everything(42)\n","        if i == 0:    \n","            prune_model(model, alpha=alpha)\n","        else:\n","            reprune_model_local(model, alpha=alpha)\n","        #if i == len(pruns) - 1 and len(pruns)>1:\n","        #    gamma = 3\n","                  \n","        scheduler = None\n","        #scheduler = sched.MultiStepLR(optimizer, milestones=[10, 20, 30], gamma=0.33)\n","        \n","        res = train(model, train_loader, optimizer, device, n_epochs = gamma*n_epochs, scheduler = scheduler)\n","        res_train += res[0]\n","        res_val += res[1]\n","        res_OOD += res[2]\n","        \n","    result = (res_train, res_val, res_OOD)\n","    #plot_text = f'prun ratio = {alpha:.2f}'\n","    if to_plot:    \n","        plot_text = ''\n","        plot_train(result, 1, plot_text)\n","        \n","    print(f'Amount of nonzero params = {count_non_zero(model):.3f}')\n","    \n","    unprune_model(model)\n","        \n","    return result"],"metadata":{"id":"qB_MGQT9db_7"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#Main part\n","where I train the model with concequativly decreasing of prunning ratio"],"metadata":{"id":"dXh7gKtEd_FO"}},{"cell_type":"markdown","source":["Performing depruning training for a different seeds to evaluate the approach at OOD data"],"metadata":{"id":"VbTdT2vD3YCQ"}},{"cell_type":"code","source":["\n","pruns = list(np.arange(0.9, -0.01, -0.1))\n","pruns = [0.95] + pruns\n","seeds = list(range(1,81))\n","#seeds = [1,2]\n","prun_losses = []\n","\n","for seed in seeds:\n","    res_tmp = train_pruning(pruns, train_loader, device, seed=seed, hidden_size = 100,\n","                            n_epochs=10, to_plot=False)[2][-100:];\n","    #print(len(res_tmp))\n","    prun_losses += res_tmp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-_TbTflrtKKt","executionInfo":{"status":"ok","timestamp":1676661719509,"user_tz":-60,"elapsed":186353,"user":{"displayName":"Timur","userId":"09007836499695604677"}},"outputId":"34c95a12-4248-4c87-af2c-35c4b953aafc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Amount of nonzero params = 0.583\n","Amount of nonzero params = 0.600\n","Amount of nonzero params = 0.708\n","Amount of nonzero params = 0.697\n","Amount of nonzero params = 0.658\n","Amount of nonzero params = 0.649\n","Amount of nonzero params = 0.618\n","Amount of nonzero params = 0.604\n","Amount of nonzero params = 0.613\n","Amount of nonzero params = 0.587\n","Amount of nonzero params = 0.609\n","Amount of nonzero params = 0.642\n","Amount of nonzero params = 0.657\n","Amount of nonzero params = 0.593\n","Amount of nonzero params = 0.626\n","Amount of nonzero params = 0.598\n","Amount of nonzero params = 0.675\n","Amount of nonzero params = 0.660\n","Amount of nonzero params = 0.604\n","Amount of nonzero params = 0.612\n","Amount of nonzero params = 0.624\n","Amount of nonzero params = 0.588\n","Amount of nonzero params = 0.629\n","Amount of nonzero params = 0.630\n","Amount of nonzero params = 0.638\n","Amount of nonzero params = 0.664\n","Amount of nonzero params = 0.630\n","Amount of nonzero params = 0.633\n","Amount of nonzero params = 0.625\n","Amount of nonzero params = 0.636\n","Amount of nonzero params = 0.647\n","Amount of nonzero params = 0.625\n","Amount of nonzero params = 0.595\n","Amount of nonzero params = 0.616\n","Amount of nonzero params = 0.633\n","Amount of nonzero params = 0.695\n","Amount of nonzero params = 0.632\n","Amount of nonzero params = 0.558\n","Amount of nonzero params = 0.645\n","Amount of nonzero params = 0.629\n","Amount of nonzero params = 0.605\n","Amount of nonzero params = 0.626\n","Amount of nonzero params = 0.605\n","Amount of nonzero params = 0.634\n","Amount of nonzero params = 0.605\n","Amount of nonzero params = 0.651\n","Amount of nonzero params = 0.626\n","Amount of nonzero params = 0.643\n","Amount of nonzero params = 0.575\n","Amount of nonzero params = 0.587\n","Amount of nonzero params = 0.612\n","Amount of nonzero params = 0.609\n","Amount of nonzero params = 0.625\n","Amount of nonzero params = 0.644\n","Amount of nonzero params = 0.622\n","Amount of nonzero params = 0.626\n","Amount of nonzero params = 0.670\n","Amount of nonzero params = 0.654\n","Amount of nonzero params = 0.609\n","Amount of nonzero params = 0.569\n","Amount of nonzero params = 0.617\n","Amount of nonzero params = 0.680\n","Amount of nonzero params = 0.640\n","Amount of nonzero params = 0.653\n","Amount of nonzero params = 0.599\n","Amount of nonzero params = 0.671\n","Amount of nonzero params = 0.625\n","Amount of nonzero params = 0.593\n","Amount of nonzero params = 0.658\n","Amount of nonzero params = 0.609\n","Amount of nonzero params = 0.673\n","Amount of nonzero params = 0.608\n","Amount of nonzero params = 0.668\n","Amount of nonzero params = 0.613\n","Amount of nonzero params = 0.614\n","Amount of nonzero params = 0.596\n","Amount of nonzero params = 0.680\n","Amount of nonzero params = 0.624\n","Amount of nonzero params = 0.658\n","Amount of nonzero params = 0.611\n"]}]},{"cell_type":"markdown","source":["Performing training of full and small models to analyze their OOD performance "],"metadata":{"id":"_4giC7hb3xPF"}},{"cell_type":"code","source":["pruns = [0]\n","\n","full_losses = []\n","\n","print('Training full size model (hidden = 200)')\n","for seed in seeds:\n","    res_tmp = train_pruning(pruns, train_loader, device, seed=seed, hidden_size = 200,\n","                            n_epochs=110, to_plot=False)[2][-100:];\n","    full_losses += res_tmp\n","\n","print('Training small model (hidden = 100)')\n","cut_losses = []\n","for seed in seeds:\n","    res_tmp = train_pruning(pruns, train_loader, device, seed=seed, hidden_size = 100,\n","                            n_epochs=110, to_plot=False)[2][-100:];\n","    cut_losses += res_tmp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"57tTe0DmIxhR","executionInfo":{"status":"ok","timestamp":1676662097050,"user_tz":-60,"elapsed":377571,"user":{"displayName":"Timur","userId":"09007836499695604677"}},"outputId":"186370ad-a6bb-4fe3-83f2-5b7d5615e1ff"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training full size model (hidden = 200)\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Training 0.5 sized model (hidden = 100)\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n","Amount of nonzero params = 1.000\n"]}]},{"cell_type":"markdown","source":["Plotting the histogrmas of the OOD losses. Cut model pruned and not pruned demonstrates similar results."],"metadata":{"id":"ma4GMTZN4FzO"}},{"cell_type":"code","source":["#plt.hist((full_losses, cut_losses, prun_losses), density=False, bins=100,\n","#          range = (0.0,2), stacked=True, label = (\"full\", \"cut\", \"prun\"))  # density=False would make counts\n","\n","plt.figure()\n","plt.hist(full_losses, bins=20, range = (0.4,1), stacked=True, histtype='step', fill=False, density = True) #, fill=False, color=\"red\"\n","plt.hist(cut_losses, bins=20, range = (0.4,1), stacked=True, histtype='step', fill=False, density = True)\n","plt.hist(prun_losses, bins=20, range = (0.4,1), stacked=True, histtype='step', fill=False, density = True)\n","plt.show()\n","\n","#plt.legend(loc='upper right')\n","#plt.ylabel('Probability')\n","#plt.xlabel('Loss');"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":267},"id":"Pam5anA3ILVE","executionInfo":{"status":"ok","timestamp":1676662230270,"user_tz":-60,"elapsed":472,"user":{"displayName":"Timur","userId":"09007836499695604677"}},"outputId":"f3803e9a-e34a-45ba-8563-39e7fc1f0c4b"},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"cell_type":"code","source":["print(np.mean(full_losses), np.std(full_losses))\n","print(np.mean(cut_losses), np.std(cut_losses))\n","print(np.mean(prun_losses), np.std(prun_losses)) "],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TmzUL_wSLamp","executionInfo":{"status":"ok","timestamp":1676654060479,"user_tz":-60,"elapsed":3,"user":{"displayName":"Timur","userId":"09007836499695604677"}},"outputId":"4a9a9ab4-1a1f-41c2-f7f4-8226eb3a2849"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["0.6139564076093511 0.11524692048700573\n","0.6632007500722843 0.14588076819374357\n","0.6400502716210699 0.14526876654842305\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"J2AVYIhEbry_"},"outputs":[],"source":["#train_model(train_loader, seed=42, hidden_size=200)\n","\n","#%%\n","\n","###\n","###\n","# Different debugs\n","###\n","###\n","\n","# model = Student(n_in = 30, n_out = 5, hidden = 200)\n","# model.to(device)\n","\n","# print(model.lin3.bias)\n","\n","# optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-2, weight_decay=0.01)\n","# #train(model, optimizer, n_epochs = 1, scheduler = None)\n","# prune_model(model, alpha=0.8)\n","\n","# print(model.lin3.bias)\n","# print(model.lin3.bias_mask)\n","\n","# train(model, optimizer, n_epochs = 1, scheduler = None)\n","\n","# print(model.lin3.bias)\n","# print(model.lin3.bias_mask)\n","\n","# reprune_model_local(model, alpha=0.2)\n","\n","# print(model.lin3.bias)\n","# print(model.lin3.bias_mask)\n","\n","\n","#%%\n","# model = Student(n_in = 30, n_out = 5, hidden = 200)\n","# old_model = Student(n_in = 30, n_out = 5, hidden = 200)\n","\n","# model.to(device)\n","\n","# # torch.save(model, 'old_model.pt')\n","\n","# #print(model.lin3.bias)\n","# prune_model(model, alpha=0.2)\n","# print(model.lin3.bias_mask)\n","# print(model.lin3.bias)\n","# print(model.lin3.bias_orig)\n","\n","# optimizer = torch.optim.AdamW(model.parameters(), lr = 1e-2, weight_decay=0.01)\n","# train(model, optimizer, n_epochs = 20, scheduler = None)\n","\n","\n","\n","# print(model.lin3.bias)\n","# print(model.lin3.bias_orig)\n","\n","# print('-----Pruning-----')\n","# reprune_model_local(model, alpha=0.2)\n","# print(model.lin3.bias_mask)\n","# print(model.lin3.bias)\n","# print(model.lin3.bias_orig)\n","\n","# print(f'Amount of nonzero params = {count_non_zero(model):.3f}')\n","\n","# #print(list((dict(model.named_buffers()).keys())))\n","\n","#%%\n","\n","# model = Student(n_in = n_in, n_out = n_out, hidden = 200)\n","# print(model)\n","# model = torch.load('teacher.pt')\n","# print(model)\n","# model.to(device)\n","\n","# loss_fun = nn.L1Loss()\n","# res_ODD = model(train_ODD.to(device))\n","# loss_ODD = loss_fun(res_ODD, target_ODD.to(device)).item()\n","\n","# print(loss_ODD)\n","\n","# print(f' number of nonzero params = {count_non_zero(model.lin3.weight):.3f}')\n","\n","\n","#%%\n","\n","'''\n","Just looking on the loss landscape of the interpolation of three and two models\n","'''\n","\n","model1 = train_model(train_loader, device, seed=42, hidden_size=200)\n","model2 = train_model(train_loader, device, seed=420, hidden_size=200)\n","model0 = Student(n_in = n_in, n_out = n_out, hidden = 100)\n","model0 = torch.load('teacher.pt')\n","model0.to(device)\n","\n","model0 = train_model(train_loader, device, seed=1420, hidden_size=200)\n","\n","model_temp = Student(n_in = n_in, n_out = n_out, hidden = 200)\n","\n","res = loss_landscape2D(model0, model1, model2, model_temp, X_train, y_train, train_ODD, target_ODD, train_ODD, target_ODD)\n","plot_landscape2D(res, n_cont=100, ODD=False)\n","\n","#%%\n","\n","losses = loss_landscape1D(model0, model2, X_train, y_train, train_ODD, target_ODD, train_ODD, target_ODD)\n","plot_landscape1D(losses)\n","\n","#%%\n","\n","model1 = train_pruning(device, seed=1)\n","model2 = train_pruning(device, seed=2)\n","\n","#%%\n","\n","losses = loss_landscape1D(model1, model2, X_train, y_train, train_ODD, target_ODD, train_ODD, target_ODD)\n","plot_landscape1D(losses)\n","\n","#%%\n","\n","'''\n","Here I tried to construct a classifier\n","'''\n","\n","num_sample = 31\n","\n","targets = [-5., -3., -1., 1., 3., 5.]\n","\n","target_samples = []\n","train_samples = []\n","\n","for target in targets:\n","    targer_sample = torch.normal(mean = target, std =  0.5, size=(num_sample, 30))\n","    train_samples.append(targer_sample)\n","    target_samples += num_sample*[target]\n","\n","train_samples = torch.cat(train_samples)\n","target_samples = torch.tensor(target_samples)\n","\n","#print(train_probs)\n","\n","# train_probs = torch.softmax(train_res, dim=1)\n","# qq = torch.argmax(train_probs, dim=1)\n","# print(qq)\n","# print(qq.float().mean().item())"]},{"cell_type":"code","source":[],"metadata":{"id":"2Cbn6_IDb6HO"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[{"file_id":"https://github.com/phlippe/uvadlc_notebooks/blob/master/docs/tutorial_notebooks/tutorial2/Introduction_to_PyTorch.ipynb","timestamp":1657228640989}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.12"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}